{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import os\n",
    "\n",
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas 0.25.3\n",
      "pytorch 1.3.1\n"
     ]
    }
   ],
   "source": [
    "print('pandas', pd.__version__)\n",
    "print('pytorch', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('gs://amazon_bucket/train_numeric_dl.tsv', sep=' ', header=None, dtype=np.int32)\n",
    "eval_df = pd.read_csv('gs://amazon_bucket/eval_numeric.tsv', sep=' ', header=None, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "      <th>478</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3521059</td>\n",
       "      <td>1189115</td>\n",
       "      <td>4352091</td>\n",
       "      <td>4664800</td>\n",
       "      <td>2091395</td>\n",
       "      <td>475580</td>\n",
       "      <td>2055385</td>\n",
       "      <td>1267201</td>\n",
       "      <td>2832609</td>\n",
       "      <td>3754422</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4570507</td>\n",
       "      <td>390981</td>\n",
       "      <td>2964667</td>\n",
       "      <td>5205699</td>\n",
       "      <td>544392</td>\n",
       "      <td>2164037</td>\n",
       "      <td>2964667</td>\n",
       "      <td>4368892</td>\n",
       "      <td>4873022</td>\n",
       "      <td>2111775</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>415758</td>\n",
       "      <td>362915</td>\n",
       "      <td>1054491</td>\n",
       "      <td>660717</td>\n",
       "      <td>3376710</td>\n",
       "      <td>4235417</td>\n",
       "      <td>572327</td>\n",
       "      <td>1552755</td>\n",
       "      <td>475580</td>\n",
       "      <td>2700358</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1735072</td>\n",
       "      <td>349092</td>\n",
       "      <td>4500667</td>\n",
       "      <td>4302042</td>\n",
       "      <td>5073616</td>\n",
       "      <td>349092</td>\n",
       "      <td>2111775</td>\n",
       "      <td>1033319</td>\n",
       "      <td>2090218</td>\n",
       "      <td>3611929</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3852213</td>\n",
       "      <td>3910364</td>\n",
       "      <td>127502</td>\n",
       "      <td>1267201</td>\n",
       "      <td>4525159</td>\n",
       "      <td>475580</td>\n",
       "      <td>2964667</td>\n",
       "      <td>2557625</td>\n",
       "      <td>390981</td>\n",
       "      <td>2964667</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 479 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2        3        4        5        6        7    \\\n",
       "0  3521059  1189115  4352091  4664800  2091395   475580  2055385  1267201   \n",
       "1  4570507   390981  2964667  5205699   544392  2164037  2964667  4368892   \n",
       "2   415758   362915  1054491   660717  3376710  4235417   572327  1552755   \n",
       "3  1735072   349092  4500667  4302042  5073616   349092  2111775  1033319   \n",
       "4  3852213  3910364   127502  1267201  4525159   475580  2964667  2557625   \n",
       "\n",
       "       8        9    ...      469      470      471      472      473  \\\n",
       "0  2832609  3754422  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "1  4873022  2111775  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "2   475580  2700358  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "3  2090218  3611929  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "4   390981  2964667  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "\n",
       "       474      475      476      477  478  \n",
       "0  5256337  5256337  5256337  5256337    5  \n",
       "1  5256337  5256337  5256337  5256337    5  \n",
       "2  5256337  5256337  5256337  5256337    5  \n",
       "3  5256337  5256337  5256337  5256337    5  \n",
       "4  5256337  5256337  5256337  5256337    5  \n",
       "\n",
       "[5 rows x 479 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(train_df.iloc[:, :-1].values, dtype=torch.long)\n",
    "X_eval_tensor = torch.tensor(eval_df.iloc[:, :-1].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = torch.tensor(train_df.iloc[:, -1].values, dtype=torch.long)\n",
    "y_eval_tensor = torch.tensor(eval_df.iloc[:, -1].values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>468</th>\n",
       "      <th>469</th>\n",
       "      <th>470</th>\n",
       "      <th>471</th>\n",
       "      <th>472</th>\n",
       "      <th>473</th>\n",
       "      <th>474</th>\n",
       "      <th>475</th>\n",
       "      <th>476</th>\n",
       "      <th>477</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3521059</td>\n",
       "      <td>1189115</td>\n",
       "      <td>4352091</td>\n",
       "      <td>4664800</td>\n",
       "      <td>2091395</td>\n",
       "      <td>475580</td>\n",
       "      <td>2055385</td>\n",
       "      <td>1267201</td>\n",
       "      <td>2832609</td>\n",
       "      <td>3754422</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4570507</td>\n",
       "      <td>390981</td>\n",
       "      <td>2964667</td>\n",
       "      <td>5205699</td>\n",
       "      <td>544392</td>\n",
       "      <td>2164037</td>\n",
       "      <td>2964667</td>\n",
       "      <td>4368892</td>\n",
       "      <td>4873022</td>\n",
       "      <td>2111775</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>415758</td>\n",
       "      <td>362915</td>\n",
       "      <td>1054491</td>\n",
       "      <td>660717</td>\n",
       "      <td>3376710</td>\n",
       "      <td>4235417</td>\n",
       "      <td>572327</td>\n",
       "      <td>1552755</td>\n",
       "      <td>475580</td>\n",
       "      <td>2700358</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1735072</td>\n",
       "      <td>349092</td>\n",
       "      <td>4500667</td>\n",
       "      <td>4302042</td>\n",
       "      <td>5073616</td>\n",
       "      <td>349092</td>\n",
       "      <td>2111775</td>\n",
       "      <td>1033319</td>\n",
       "      <td>2090218</td>\n",
       "      <td>3611929</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3852213</td>\n",
       "      <td>3910364</td>\n",
       "      <td>127502</td>\n",
       "      <td>1267201</td>\n",
       "      <td>4525159</td>\n",
       "      <td>475580</td>\n",
       "      <td>2964667</td>\n",
       "      <td>2557625</td>\n",
       "      <td>390981</td>\n",
       "      <td>2964667</td>\n",
       "      <td>...</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "      <td>5256337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 478 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0        1        2        3        4        5        6        7    \\\n",
       "0  3521059  1189115  4352091  4664800  2091395   475580  2055385  1267201   \n",
       "1  4570507   390981  2964667  5205699   544392  2164037  2964667  4368892   \n",
       "2   415758   362915  1054491   660717  3376710  4235417   572327  1552755   \n",
       "3  1735072   349092  4500667  4302042  5073616   349092  2111775  1033319   \n",
       "4  3852213  3910364   127502  1267201  4525159   475580  2964667  2557625   \n",
       "\n",
       "       8        9    ...      468      469      470      471      472  \\\n",
       "0  2832609  3754422  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "1  4873022  2111775  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "2   475580  2700358  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "3  2090218  3611929  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "4   390981  2964667  ...  5256337  5256337  5256337  5256337  5256337   \n",
       "\n",
       "       473      474      475      476      477  \n",
       "0  5256337  5256337  5256337  5256337  5256337  \n",
       "1  5256337  5256337  5256337  5256337  5256337  \n",
       "2  5256337  5256337  5256337  5256337  5256337  \n",
       "3  5256337  5256337  5256337  5256337  5256337  \n",
       "4  5256337  5256337  5256337  5256337  5256337  \n",
       "\n",
       "[5 rows x 478 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[:5, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1290927, 478])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eval_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor -= 1  # To aid with cross-entropy loss.\n",
    "y_eval_tensor -= 1\n",
    "assert y_train_tensor.max() == 4 and y_train_tensor.min() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_df, eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "n_classes = set(y_train_tensor.tolist())\n",
    "print(len(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = F.one_hot(y_train_tensor.view(-1, 1), num_classes=len(n_classes))\n",
    "y_eval_tensor = F.one_hot(y_eval_tensor.view(-1, 1), num_classes=len(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_true):\n",
    "    _, y_pred_indices = y_pred.max(dim=1)\n",
    "    n_correct = torch.eq(y_pred_indices, y_true).sum().item()\n",
    "    return torch.tensor(n_correct / y_pred_indices.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_learning_rate(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weight(series):\n",
    "    counts = Counter(series)\n",
    "    weights = {k: (v / len(series)) for k, v in counts.items()}\n",
    "    weight_tuples = sorted([(k, v) for k, v in weights.items()])\n",
    "    sorted_weights = [weight for _, weight in weight_tuples]\n",
    "    return torch.tensor(sorted_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_padding_index(padding_value, vocab):\n",
    "    \"\"\"Takes a string and returns an int which corresponds to the padding index.\"\"\"\n",
    "    return vocab[padding_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab(file_path):\n",
    "    \"\"\"Loads a json file and converts it to a dictionary.\"\"\"\n",
    "    with open(file_path, 'r') as d:\n",
    "        out = json.load(d)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client()\n",
    "bucket = client.get_bucket('amazon_bucket')\n",
    "blob = bucket.get_blob('word_to_index.json')\n",
    "json_data = blob.download_to_filename('test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.json') as jd:\n",
    "    word_to_index = json.load(jd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = compute_class_weight(y_train_tensor.squeeze().max(dim=1).indices.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_index = get_padding_index('<pad>', word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_torch = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "eval_torch = torch.utils.data.TensorDataset(X_eval_tensor, y_eval_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModelMLP(pl.LightningModule):\n",
    "    def __init__(self, vocab_size=len(word_to_index.keys()), embedding_size=100,\n",
    "               dropout=0.25, num_classes=len(weights),\n",
    "               h1=512, h2=256, h3=128, h4=32):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                       embedding_dim=embedding_size,\n",
    "                                       padding_idx = padding_index,\n",
    "                                       sparse=False)\n",
    "        self.linear1 = nn.Linear(in_features=embedding_size, out_features=h1)\n",
    "        self.linear2 = nn.Linear(in_features=h1, out_features=h2)\n",
    "        self.linear3 = nn.Linear(in_features=h2, out_features=h3)\n",
    "        self.linear4 = nn.Linear(in_features=h3, out_features=h4)\n",
    "        self.out = nn.Linear(in_features=h4, out_features=num_classes)\n",
    "        self.dropout_p = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(self.embeddings(x)))\n",
    "        x = F.dropout(x, p=self.dropout_p)\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.dropout(x, p=self.dropout_p)\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.dropout(x, p=self.dropout_p)\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = F.dropout(x, p=self.dropout_p)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(train_torch, batch_size=64,\n",
    "                                           shuffle=False, pin_memory=True,\n",
    "                                           drop_last=True, num_workers=4)\n",
    "        \n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(eval_torch, batch_size=64,\n",
    "                                           shuffle=False, pin_memory=True,\n",
    "                                           drop_last=True, num_workers=4)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-5)\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        x = x.view(1024, 478)\n",
    "        y = y.view(1024, 5)\n",
    "        y_pred = self.forward(x)\n",
    "        acc_t = compute_accuracy(y_pred, y)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        self.logger.experiment.log(\n",
    "            {'loss': loss.item(),\n",
    "             'train_acc': acc_t.item(),\n",
    "             'batch_nb': batch_nb})\n",
    "        return {'loss': loss, 'train_acc': acc_t}\n",
    "\n",
    "    def training_end(self, outputs):\n",
    "        average_loss = torch.stack([x['loss'] for x in outputs]).mean()\n",
    "        average_accuracy = torch.stack([x['train_acc'] for x in outputs]).mean()\n",
    "        return {'mean_train_loss': average_loss,\n",
    "                'mean_train_accuracy': average_accuracy}\n",
    "\n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        x, y = batch\n",
    "        print(x.size())\n",
    "        x = x.view(1024, 478)\n",
    "        y = y.view(1024, 5)\n",
    "        y_pred = self.forward(x)\n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        acc_t = compute_accuracy(y_pred, y)\n",
    "        self.logger.experiment.log(\n",
    "            {'val_loss': loss.item(),\n",
    "             'val_acc': acc_t.item(),\n",
    "             'batch_nb': batch_nb})\n",
    "        return {'val_loss': loss, 'val_acc': acc_t}\n",
    "\n",
    "    def validation_end(self, outputs):\n",
    "        average_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        average_accuracy = \\\n",
    "            torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        return {'mean_val_loss': average_loss, \n",
    "                'mean_val_accuracy': average_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    filepath=f'{os.getcwd()}/mlp_model', save_best_only=True, verbose=True,\n",
    "    monitor='mean_val_loss', mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = pl.callbacks.EarlyStopping(\n",
    "    monitor='mean_val_loss', min_delta=0.0005, patience=3, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = pl.logging.TestTubeLogger(\n",
    "    save_dir=f'{os.getcwd()}/train_logs/mlp_model', name='mlp_model'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(logger=logger, checkpoint_callback=checkpoint_callback,\n",
    "                     max_nb_epochs=25, min_nb_epochs=10, gpus=None, \n",
    "                     early_stop_callback=early_stopping_callback,\n",
    "                     fast_dev_run=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextModelMLP(\n",
       "  (embeddings): Embedding(5490692, 100, padding_idx=5256337)\n",
       "  (linear1): Linear(in_features=100, out_features=512, bias=True)\n",
       "  (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (linear3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (linear4): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (out): Linear(in_features=32, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = TextModelMLP()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?batch/s]"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-45d4afebefac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# return 1 when finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_progress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_val_dataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_sanity_val_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0;31m# close progress bars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/pytorch_lightning/trainer/evaluation_loop_mixin.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, dataloaders, max_batches, test)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mdl_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/torch/lib/python3.7/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
